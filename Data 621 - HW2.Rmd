---
title: "Data 621 - Homework 2"
author: "Group 1"
date: "October, 2019"
output:
  html_document:
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 2
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '2'    
---

# Overview

In this homework assignment, we are to work through various classification metrics. 

## Objectives
- To create functions in R to carry out the various calculations
- To investigate some functions in packages that will obtain the equivalent results
- To create graphical output that can be used to evaluate the output of classification models, such as binary logistic regression.

## Import data
```{r warning=FALSE, message=FALSE}
library("knitr")
class_data <- read.csv("classification-output-data.csv")
attach(class_data)

dt <- c(scored.class, class)
```

# Write R Functions

## Get the raw confusion matrix

```{r echo=FALSE, warning=FALSE, message=FALSE}
tbl <- table(scored.class, class)

kable(tbl)
tn <- tbl[2,2]
tp <- tbl[1,1]
fp <- tbl[1,2]
fn <- tbl[2,1]
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
print(paste0("True positive (TP) is: ", tp))
print(paste0("True negative (TN) is: ", tn))
print(paste0("False positive (FP) is: ", fp))
print(paste0("False negative (FN) is: ", fn))
```

## Accuracy of the Predictions

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions.

Accuracy : the proportion of the total number of predictions that were correct.

$$Accuracy = \frac {(TP + TN)}{(TP + FP + TN + FN)}$$

```{r warning=FALSE, message=FALSE}
prd_accuracy <- function () {
    accuracy = round((tp + tn) / (tp + fp + tn + fn),4)
    return (accuracy)
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- prd_accuracy()
print(paste0("The prediction accuracy is: ", result))
```

## Classification Error Rate of the Predictions

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions.

$$Classification Error Rate=\frac {FP+FN}{TP+FP+TN+FN}$$
```{r warning=FALSE, message=FALSE}
prd_class_error <- function () {
    class_err_rate = round((fp + fn) / (tp + fp + tn + fn),4)
    return (class_err_rate)
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- prd_class_error()
print(paste0("The classification error rate of the prediction is: ", result))
```

## Precision of the Predictions
Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the precision of the predictions.

$$Precision = \frac {TP}{TP + FP}$$

```{r warning=FALSE, message=FALSE}
prd_precision <- function () {
    precision = round(tp / (tp + fp),4)
    return (precision)
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- prd_precision()
print(paste0("The prediction precision is: ", result))
```

## Sensitivity of the Predictions
Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the sensitivity of the predictions. Sensitivity is also known as recall.

$$Sensitivity = \frac {TP}{TP + FN}$$

Sensitivity or Recall : the proportion of actual positive cases which are correctly identified.

```{r warning=FALSE, message=FALSE}
prd_recall <- function () {
    recall = round(tp / (tp + fn),4)
    return (recall)
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- prd_recall()
print(paste0("The prediction sensitivity is: ", result))
```

## Specificity of the Predictions
Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the specificity of the predictions.

$$Specificity = \frac {TN}{TN + FP}$$

```{r warning=FALSE, message=FALSE}
prd_specificity <- function () {
    specificity = round(tn / (tn + fp),4)
    return (specificity)
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- prd_specificity()
print(paste0("The prediction specificity is: ", result))
```

## F1 Score of the Predictions
Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the F1 score of the predictions.

$$F_1 Score = \frac {2 * Precision * Sensitivity}{Precision + Sensitivity} $$

```{r warning=FALSE, message=FALSE}
prd_f1_score <- function () {
    f1_score = round((2*prd_precision()*prd_recall()) / (prd_precision()+prd_recall()),4)
    return (f1_score)
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- prd_f1_score()
print(paste0("The F1 score of the prediction is: ", result))
```

## All Classification Metrics
Use your created R functions and the provided classification output data set to produce all of the classification metrics discussed above.

```{r warning=FALSE, message=FALSE}

Name <- c('Accuracy','Classification Error Rate', 'Precision', 'Sensitivity','Specificity', 'F1 Score')
Value <- round(c(prd_accuracy(), prd_class_error(), prd_precision(), prd_recall(), prd_specificity(), prd_f1_score()),4)
df1 <- as.data.frame(cbind(Name, Value))
kable(df1)
```
