---
title: "Data 621 - Homework 2"
author: "Group 1"
date: "October, 2019"
output:
  html_document:
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 2
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '2'    
---

# Overview

In this homework assignment, we are to work through various classification metrics. 

## Objectives
- To create functions in R to carry out the various calculations
- To investigate some functions in packages that will obtain the equivalent results
- To create graphical output that can be used to evaluate the output of classification models, such as binary logistic regression.

## Import data
```{r warning=FALSE, message=FALSE}
library("knitr")
library("ggplot2")
library(pROC)
class_data <- read.csv("classification-output-data.csv")
attach(class_data)

dt <- c(scored.class, class)
```

# Write R Functions

## Get the raw confusion matrix

```{r echo=FALSE, warning=FALSE, message=FALSE}
tbl <- table(scored.class, class)

kable(tbl)
tn <- tbl[2,2]
tp <- tbl[1,1]
fp <- tbl[1,2]
fn <- tbl[2,1]
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
print(paste0("True positive (TP) is: ", tp))
print(paste0("True negative (TN) is: ", tn))
print(paste0("False positive (FP) is: ", fp))
print(paste0("False negative (FN) is: ", fn))
```

## Accuracy of the Predictions

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions.

Accuracy : the proportion of the total number of predictions that were correct.

$$Accuracy = \frac {(TP + TN)}{(TP + FP + TN + FN)}$$

```{r warning=FALSE, message=FALSE}
prd_accuracy <- function () {
    accuracy = round((tp + tn) / (tp + fp + tn + fn),4)
    return (accuracy)
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- prd_accuracy()
print(paste0("The prediction accuracy is: ", result))
```

## Classification Error Rate of the Predictions

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions.

$$Classification Error Rate=\frac {FP+FN}{TP+FP+TN+FN}$$
```{r warning=FALSE, message=FALSE}
prd_class_error <- function () {
    class_err_rate = round((fp + fn) / (tp + fp + tn + fn),4)
    return (class_err_rate)
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- prd_class_error()
print(paste0("The classification error rate of the prediction is: ", result))
```

## Precision of the Predictions
Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the precision of the predictions.

$$Precision = \frac {TP}{TP + FP}$$

```{r warning=FALSE, message=FALSE}
prd_precision <- function () {
    precision = round(tp / (tp + fp),4)
    return (precision)
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- prd_precision()
print(paste0("The prediction precision is: ", result))
```

## Sensitivity of the Predictions
Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the sensitivity of the predictions. Sensitivity is also known as recall.

$$Sensitivity = \frac {TP}{TP + FN}$$

Sensitivity or Recall : the proportion of actual positive cases which are correctly identified.

```{r warning=FALSE, message=FALSE}
prd_recall <- function () {
    recall = round(tp / (tp + fn),4)
    return (recall)
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- prd_recall()
print(paste0("The prediction sensitivity is: ", result))
```

## Specificity of the Predictions
Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the specificity of the predictions.

$$Specificity = \frac {TN}{TN + FP}$$

```{r warning=FALSE, message=FALSE}
prd_specificity <- function () {
    specificity = round(tn / (tn + fp),4)
    return (specificity)
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- prd_specificity()
print(paste0("The prediction specificity is: ", result))
```

## F1 Score of the Predictions
Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the F1 score of the predictions.

$$F_1 Score = \frac {2 * Precision * Sensitivity}{Precision + Sensitivity} $$

```{r warning=FALSE, message=FALSE}
prd_f1_score <- function () {
    f1_score = round((2*prd_precision()*prd_recall()) / (prd_precision()+prd_recall()),4)
    return (f1_score)
}
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
result <- prd_f1_score()
print(paste0("The F1 score of the prediction is: ", result))
```

## Plot the ROC Curve and Compute the AUC

Below is the plot of the Receiver Operating Characteristic (ROC) curve, as well as the results of a function computed the area under a curve (AUC).

```{r, echo=FALSE, warning=FALSE}
# computeROCAUC <- function(df, class, probability)
# {
#     df <- df[order(df[probability], decreasing = TRUE),]
#     response <- as.numeric(unlist(c(0,df[class])))
#     x <- cumsum(df[class] == 0)/57
#     y <- cumsum(df[class] == 1)/57
#     # plot from ROC package
#     #plot(x, y, type = 'l')
#     g <- ggplot(data = df, aes(x, y))  + geom_line()
#  
#     area <- cumsum(df$class)/57  * 1/181
# 
#     
#     return(list(g,area))
# }
computeROCAUC <- function(df, class, probability)
{
    #df <- rbind(df, data.frame(class = 0, scored.class = 0, scored.probability = 0))
    df <- df[order(df[probability], decreasing = FALSE),]
    y <- cumsum(df[class] == 0)/sum(df[class]==0)
    df <- df[order(df[probability], decreasing = TRUE),]
    x = cumsum(df[class] == 1)/sum(df[class]!=0)
    x = 1- x[order(x, decreasing = TRUE)]
    dfXY <- data.frame(x= x, y=y)
    g <- ggplot(data = dfXY, aes(x, y))  + geom_line()
    
    #compute area by adding up all the individual slices of area
    xPrev <- x[1]
    area <- 0
    for(i in seq(2,length(x)-1))
    {
        if(x[i] != xPrev)
        {
            area <- area + ((x[i] - xPrev) * y[i])
            xPrev <- x[i]
        }
    }
    area <- area + ((x[length(x)] - xPrev) * y[length(x)] )
    
    
    return(list(g,area))
}
```

```{r, echo=FALSE, warning=FALSE}
rocResults <- computeROCAUC(class_data, "class", "scored.probability")
rocResults[[1]]
print(paste("The area under the curve was computed as: ", round(sum(unlist(rocResults[2])),4)))
```

## All Classification Metrics
Use your created R functions and the provided classification output data set to produce all of the classification metrics discussed above.

```{r warning=FALSE, message=FALSE}

Name <- c('Accuracy','Classification Error Rate', 'Precision', 'Sensitivity','Specificity', 'F1 Score')
Value <- round(c(prd_accuracy(), prd_class_error(), prd_precision(), prd_recall(), prd_specificity(), prd_f1_score()),4)
df1 <- as.data.frame(cbind(Name, Value))
kable(df1)
```

## Investigate the pROC Package

The pRoc package simplifies the generation of ROC curves. Below are two plots, one plotted with pROC and one plotted with the data from our function.  Note they match essentially exactly.

We can compare the sensitivity and s

```{r warning=FALSE, message=FALSE}
rocPkg <- roc(class ~ scored.probability, data = class_data)
#plot(rocPkg$sensitivities, rocPkg$specificities , legacy.axes = TRUE, xlim=range(0,1))
g1 <- ggplot(data.frame(sensitivities = rocPkg$sensitivities, specificities = rocPkg$specificities), aes(1-sensitivities, specificities))  +
    geom_line() +
    ggtitle("Graph from Data from the pROC Package")

require(gridExtra)

g2 <- rocResults[[1]] + # graph from above
    ggtitle("Graph using data from our function.")

gridExtra::grid.arrange(g1,g2)

```
